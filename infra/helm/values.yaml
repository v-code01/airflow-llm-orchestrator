# Helm values for AirflowLLM deployment

airflow:
  image:
    repository: ghcr.io/v-code01/airflow-llm
    tag: latest
    pullPolicy: Always

  executor: KubernetesExecutor

  config:
    AIRFLOW__CORE__PARALLELISM: "64"
    AIRFLOW__CORE__DAG_CONCURRENCY: "32"
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: "4"
    AIRFLOW__KUBERNETES__DELETE_WORKER_PODS: "True"
    AIRFLOW__KUBERNETES__DELETE_WORKER_PODS_ON_FAILURE: "False"
    AIRFLOW_LLM__COST_OPTIMIZATION: "true"
    AIRFLOW_LLM__SELF_HEALING: "true"
    AIRFLOW_LLM__PREDICTIVE_ANALYTICS: "true"

  webserver:
    replicas: 3
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 2
        memory: 4Gi

    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 10
      targetCPUUtilizationPercentage: 70

  scheduler:
    replicas: 2
    resources:
      requests:
        cpu: 2
        memory: 4Gi
      limits:
        cpu: 4
        memory: 8Gi

  workers:
    replicas: 10
    resources:
      requests:
        cpu: 2
        memory: 4Gi
      limits:
        cpu: 4
        memory: 8Gi

    # Node selectors for different workload types
    nodeSelector:
      default:
        NodeType: general

      gpu_tasks:
        NodeType: gpu

      cost_optimized:
        NodeType: spot

    tolerations:
      spot:
        - key: spot
          operator: Equal
          value: "true"
          effect: NoSchedule

      gpu:
        - key: nvidia.com/gpu
          operator: Equal
          value: "true"
          effect: NoSchedule

  postgresql:
    enabled: false # Using external RDS
    externalDatabase:
      host: ${RDS_ENDPOINT}
      port: 5432
      database: airflow
      user: airflow
      passwordSecret: airflow-db-password
      passwordSecretKey: password

  redis:
    enabled: false # Using external ElastiCache
    externalRedis:
      host: ${REDIS_ENDPOINT}
      port: 6379

  serviceAccount:
    create: true
    name: airflow-llm
    annotations:
      eks.amazonaws.com/role-arn: ${AIRFLOW_POD_ROLE_ARN}

# AirflowLLM specific configurations
airflowLLM:
  costOptimization:
    enabled: true
    providers:
      - aws
      - gcp
      - azure
      - coreweave

    spotInstance:
      enabled: true
      maxSpotPrice: 0.5
      interruptionBehavior: terminate

    multiCloud:
      enabled: true
      preferredProvider: aws
      fallbackProviders:
        - gcp
        - azure

  selfHealing:
    enabled: true
    maxRetries: 3
    errorPatterns:
      - type: ImportError
        autoFix: true
        strategy: install_package

      - type: MemoryError
        autoFix: true
        strategy: scale_resources

      - type: ConnectionError
        autoFix: true
        strategy: exponential_backoff

  llmModels:
    providers:
      - name: openai
        models:
          - gpt-4
          - gpt-3.5-turbo
        apiKeySecret: api-keys
        apiKeySecretKey: openai_api_key

      - name: anthropic
        models:
          - claude-3-opus
          - claude-3-sonnet
        apiKeySecret: api-keys
        apiKeySecretKey: anthropic_api_key

      - name: local
        models:
          - llama-70b
        endpoint: http://llama-service:8080

  monitoring:
    prometheus:
      enabled: true
      serviceMonitor:
        enabled: true
        interval: 30s

    grafana:
      enabled: true
      dashboards:
        - airflow-llm-overview
        - cost-optimization
        - self-healing-metrics
        - llm-usage

  logging:
    level: INFO
    structured: true
    outputs:
      - stdout
      - elasticsearch

    elasticsearch:
      host: elasticsearch.monitoring.svc.cluster.local
      port: 9200
      index: airflow-llm

# Ingress configuration
ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"

  hosts:
    - host: airflow-llm.example.com
      paths:
        - path: /
          pathType: Prefix

  tls:
    - secretName: airflow-llm-tls
      hosts:
        - airflow-llm.example.com

# Resource quotas and limits
resourceQuota:
  enabled: true
  hard:
    requests.cpu: "1000"
    requests.memory: "4Ti"
    requests.storage: "10Ti"
    persistentvolumeclaims: "100"
    pods: "1000"

# Pod disruption budgets
podDisruptionBudget:
  webserver:
    minAvailable: 1

  scheduler:
    minAvailable: 1

  worker:
    minAvailable: 5

# Network policies
networkPolicy:
  enabled: true
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: airflow-llm
        - podSelector:
            matchLabels:
              app: airflow-llm
      ports:
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 5555
