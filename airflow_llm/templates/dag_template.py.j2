"""
{{ description }}
Generated by AirflowLLM
"""
from datetime import datetime, timedelta
import logging
import os
from typing import Dict

import airflow
from airflow.models.baseoperator import BaseOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.sensors.sql import SqlSensor
import yaml

# Import AirflowLLM components
try:
    from airflow_llm.decorators import (
        self_healing_task,
        cost_aware_execution,
        performance_monitor,
        intelligent_retry
    )
    from airflow_llm import config_parser
    from airflow_llm import utils as general_utils
    AIRFLOW_LLM_AVAILABLE = True
except ImportError:
    AIRFLOW_LLM_AVAILABLE = False

# DAG Configuration
DAG_ID = "{{ dag_id }}"
DAG_RESOURCES_DIR = os.path.join("dags", "resources", DAG_ID)
SQL_DIR = os.path.join("resources", DAG_ID)

LOGGER = logging.getLogger(__name__)

# Default arguments with AirflowLLM enhancements
DEFAULT_ARGS = {
    "owner": "{{ owners }}",
    "email": ["airflow@company.com"],
    "email_on_failure": {{ email_on_failure | lower }},
    "email_on_retry": False,
    "retries": {{ retries }},
    "retry_delay": timedelta(minutes=5),
    "executor_config": {},
    "mode": "reschedule",
    "poke_interval": 900,
    "sla": timedelta(hours=10),
}

# Create DAG with AirflowLLM features
DAG = airflow.DAG(
    dag_id=DAG_ID,
    description="{{ description }}",
    max_active_runs={{ max_active_runs }},
    start_date=datetime({{ start_date }}),
    schedule_interval="{{ schedule_interval }}",
    default_args=DEFAULT_ARGS,
    catchup={{ catchup | lower }},
    tags={{ tags | tojson }},
)


def attach_tasks_to_dag(dag: airflow.DAG) -> airflow.DAG:
    """Create and assign all tasks necessary for the given DAG following datasci-rx patterns.

    Arguments:
        dag (airflow.models.DAG): the DAG to attach the tasks to.

    Returns:
        airflow.models.DAG: ``dag``
    """

    task_list: Dict[str, BaseOperator] = {}
    dag_config_path = os.path.join(DAG_RESOURCES_DIR, "tasks.yml")

    with open(dag_config_path) as dag_config_file:
        dag_config = yaml.safe_load(dag_config_file)

    # Set up environment variables
    var_config_path = os.path.join(DAG_RESOURCES_DIR, "config.yml")
    if AIRFLOW_LLM_AVAILABLE:
        env_vars = config_parser.parse_config_yaml(var_config_path)
    else:
        with open(var_config_path) as f:
            env_vars = yaml.safe_load(f)
            deployment_env = os.getenv("DEPLOYMENT_ENVIRONMENT", "dev")
            for k, v in env_vars.items():
                if isinstance(v, dict) and deployment_env in v:
                    env_vars[k] = v[deployment_env]

    for task in dag_config["tasks"]:
        task_operator = None

        # Enhanced task creation with AirflowLLM decorators
        if task["operator"] == "postgresql":
            sql_params = {
                "iam_role_s3": env_vars["iam_role_s3"],
                "s3_bucket": env_vars["s3_bucket"],
                "output_schema": env_vars["output_schema"],
                "stakeholders": env_vars["stakeholders"],
                "owners": env_vars["owners"],
            }

            @self_healing_task(retries=3, auto_fix=True)
            @cost_aware_execution(max_cost_per_hour=2.0, prefer_spot_instances=True)
            @performance_monitor(track_memory=True, track_cpu=True)
            def create_postgres_operator():
                return PostgresOperator(
                    task_id=task["name"],
                    postgres_conn_id="redshift_default",
                    sql=os.path.join(SQL_DIR, task["sql_script"]),
                    trigger_rule="all_success",
                    params=sql_params,
                    dag=dag,
                )

            task_operator = create_postgres_operator()

        elif task["operator"] == "python":
            # Import the Python function for this task
            python_callable_name = task.get("python_callable")
            if python_callable_name:
                # Dynamically import the function
                module_path = f"dags.resources.{DAG_ID}.{python_callable_name}"
                try:
                    import importlib
                    module = importlib.import_module(module_path)
                    python_callable = getattr(module, python_callable_name.replace("_function", ""))
                except ImportError:
                    LOGGER.warning(f"Could not import {module_path}, using default function")
                    python_callable = lambda **context: {"status": "completed"}
            else:
                python_callable = lambda **context: {"status": "completed"}

            @self_healing_task(retries=2, auto_fix=True, resource_scaling=True)
            @cost_aware_execution(max_cost_per_hour=1.5, prefer_spot_instances=True)
            @intelligent_retry(max_retries=3, backoff_factor=2.0)
            @performance_monitor(track_memory=True, track_cpu=True)
            def create_python_operator():
                return PythonOperator(
                    task_id=task["name"],
                    python_callable=python_callable,
                    dag=dag,
                    pool=task.get("pool"),
                    retries=task.get("retries", DEFAULT_ARGS["retries"])
                )

            task_operator = create_python_operator()

        elif task["operator"] == "sensor":
            # Default sensor SQL or use custom from task config
            sensor_sql = task.get("sql", """
                SELECT COUNT(*) FROM information_schema.tables
                WHERE table_name = 'default_table'
                HAVING COUNT(*) > 0
            """)

            @self_healing_task(retries=5, auto_fix=True)
            @intelligent_retry(max_retries=10, backoff_factor=1.5)
            def create_sensor_operator():
                return SqlSensor(
                    dag=dag,
                    task_id=task["name"],
                    conn_id=task.get("connection_id", "redshift_default"),
                    sql=sensor_sql,
                    retries=task.get("retries", 5),
                    poke_interval=task.get("poke_interval", 300),
                    timeout=task.get("timeout", 3600),
                )

            task_operator = create_sensor_operator()

        elif task["operator"] == "dummy":
            task_operator = DummyOperator(dag=dag, task_id=task["name"])

        # Set up task dependencies
        if task_operator:
            for dependency in task["depends_on"]:
                if dependency == "none":
                    break
                if dependency in task_list:
                    task_list[dependency] >> task_operator

            task_list[task["name"]] = task_operator

    return dag


# Enhanced monitoring and alerting functions
def on_failure_callback(context):
    """Enhanced failure callback with AirflowLLM self-healing"""
    LOGGER.error(f"Task {context['task_instance'].task_id} failed")

    # Try self-healing if available
    try:
        from airflow_llm.self_healer import SelfHealingAgent
        healer = SelfHealingAgent()

        error = context.get('exception')
        if error:
            analysis = healer.analyze_error(error, context)
            if analysis.auto_fixable:
                LOGGER.info(f"Attempting auto-fix: {analysis.suggested_fix}")
                # Apply fix and retry
                return analysis.apply_fix()
    except Exception as e:
        LOGGER.warning(f"Self-healing failed: {e}")

    # Default failure handling
    slack.send_alert(f"DAG {DAG_ID} task failure", context)


def on_success_callback(context):
    """Enhanced success callback with cost tracking"""
    LOGGER.info(f"Task {context['task_instance'].task_id} completed successfully")

    # Track cost and performance metrics
    try:
        from airflow_llm.cost_optimizer import CostTracker
        cost_tracker = CostTracker()

        cost = cost_tracker.calculate_task_cost(context['task_instance'])
        LOGGER.info(f"Task cost: ${cost:.4f}")

        # Store metrics for optimization
        context['ti'].xcom_push(key='execution_cost', value=cost)

    except Exception as e:
        LOGGER.warning(f"Cost tracking failed: {e}")


# Apply callbacks to DAG
DAG.on_failure_callback = on_failure_callback
DAG.on_success_callback = on_success_callback

# Attach tasks to DAG
if not os.getenv("TEST_ENVIRONMENT"):
    attach_tasks_to_dag(DAG)

# Export for Airflow discovery
globals()[DAG_ID] = DAG
